
"""
================================================================================
PROJET DATA SCIENCE & MACHINE LEARNING
CREDIT SCORING - "GIVE ME SOME CREDIT"
================================================================================

Date: DÃ©cembre 2025
Dataset: Kaggle - Give Me Some Credit (2011)

Objectif: PrÃ©dire la probabilitÃ© qu'un client connaisse des difficultÃ©s 
          financiÃ¨res dans les 2 prochaines annÃ©es.

Type de problÃ¨me: Classification Binaire
================================================================================
"""

# =============================================================================
# PARTIE 1 : CHARGEMENT ET EXPLORATION DES DONNÃ‰ES (EDA)
# =============================================================================

print("="*80)
print("PARTIE 1 : EXPLORATION DES DONNÃ‰ES")
print("="*80)

# -----------------------------------------------------------------------------
# 1.1 Importation des bibliothÃ¨ques
# -----------------------------------------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# BibliothÃ¨ques ML
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, 
                             roc_curve, f1_score, accuracy_score, precision_score, recall_score)

# Pour gÃ©rer le dÃ©sÃ©quilibre des classes
from imblearn.over_sampling import SMOTE

# Configuration
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)
np.random.seed(42)

print("âœ… BibliothÃ¨ques importÃ©es avec succÃ¨s\n")

# -----------------------------------------------------------------------------
# 1.2 Chargement des donnÃ©es
# -----------------------------------------------------------------------------

# Chargez le fichier CSV (adaptez le chemin selon votre structure)
df = pd.read_csv('../data/raw/cs-training.csv', index_col=0)

print(f"ðŸ“Š Dimensions du dataset : {df.shape[0]:,} lignes Ã— {df.shape[1]} colonnes")
print(f"ðŸ’¾ Taille mÃ©moire : {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n")

# AperÃ§u des premiÃ¨res lignes
print("ðŸ” AperÃ§u des donnÃ©es :")
print(df.head())

# -----------------------------------------------------------------------------
# 1.3 Informations sur le dataset
# -----------------------------------------------------------------------------

print("\nðŸ“‹ Informations sur les colonnes :")
print(df.info())

print("\nðŸ“Š Statistiques descriptives :")
print(df.describe().T)

# ðŸ’¡ INTERPRÃ‰TATION
print("""
ðŸ’¡ OBSERVATIONS INITIALES :
- Le dataset contient 150,000 observations et 11 variables
- PrÃ©sence de valeurs manquantes dans MonthlyIncome et NumberOfDependents
- Plusieurs variables prÃ©sentent des valeurs extrÃªmes (outliers)
- Les variables de retard de paiement sont des compteurs (valeurs discrÃ¨tes)
""")

# -----------------------------------------------------------------------------
# 1.4 Analyse des valeurs manquantes
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("âš ï¸  ANALYSE DES VALEURS MANQUANTES")
print("="*80)

missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Valeurs Manquantes': missing,
    'Pourcentage (%)': missing_pct
}).sort_values('Pourcentage (%)', ascending=False)

print(missing_df[missing_df['Valeurs Manquantes'] > 0])

# Visualisation
plt.figure(figsize=(10, 5))
missing_df[missing_df['Valeurs Manquantes'] > 0].plot(kind='bar', y='Pourcentage (%)', color='coral')
plt.title('Pourcentage de Valeurs Manquantes par Variable', fontsize=14, fontweight='bold')
plt.xlabel('Variables')
plt.ylabel('Pourcentage (%)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- MonthlyIncome : 19.8% de valeurs manquantes â†’ Imputation nÃ©cessaire
- NumberOfDependents : 2.6% de valeurs manquantes â†’ Imputation par mÃ©diane possible
- Ces valeurs manquantes peuvent Ãªtre informatives (clients n'ayant pas dÃ©clarÃ© leur revenu)
""")

# -----------------------------------------------------------------------------
# 1.5 Analyse de la variable cible
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("ðŸŽ¯ ANALYSE DE LA VARIABLE CIBLE")
print("="*80)

target_counts = df['SeriousDlqin2yrs'].value_counts()
target_pct = df['SeriousDlqin2yrs'].value_counts(normalize=True) * 100

print(f"\nDistribution de la Target :")
print(f"  â€¢ Classe 0 (Pas de dÃ©faut) : {target_counts[0]:,} ({target_pct[0]:.2f}%)")
print(f"  â€¢ Classe 1 (DÃ©faut)        : {target_counts[1]:,} ({target_pct[1]:.2f}%)")
print(f"\nâš ï¸  Dataset DÃ‰SÃ‰QUILIBRÃ‰ : ratio {target_pct[1]:.2f}% de classe positive")

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

target_counts.plot(kind='bar', ax=axes[0], color=['lightgreen', 'salmon'])
axes[0].set_title('Distribution de la Variable Cible', fontsize=14, fontweight='bold')
axes[0].set_xlabel('SeriousDlqin2yrs')
axes[0].set_ylabel('Nombre de clients')
axes[0].set_xticklabels(['Pas de dÃ©faut (0)', 'DÃ©faut (1)'], rotation=0)

axes[1].pie(target_counts, labels=['Pas de dÃ©faut', 'DÃ©faut'], autopct='%1.1f%%', 
            colors=['lightgreen', 'salmon'], startangle=90)
axes[1].set_title('Proportion des Classes', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Fort dÃ©sÃ©quilibre des classes (93% vs 7%)
- NÃ©cessitÃ© d'utiliser SMOTE ou class_weight pour rÃ©Ã©quilibrer
- MÃ©triques Ã  privilÃ©gier : F1-Score, ROC-AUC, Recall (pas seulement Accuracy)
""")

# -----------------------------------------------------------------------------
# 1.6 Distribution des variables numÃ©riques
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("ðŸ“ˆ DISTRIBUTION DES VARIABLES NUMÃ‰RIQUES")
print("="*80)

numeric_cols = df.select_dtypes(include=[np.number]).columns.drop('SeriousDlqin2yrs')

fig, axes = plt.subplots(5, 2, figsize=(15, 18))
axes = axes.ravel()

for idx, col in enumerate(numeric_cols):
    axes[idx].hist(df[col].dropna(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'Distribution: {col}', fontsize=11, fontweight='bold')
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('FrÃ©quence')
    axes[idx].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Plusieurs variables sont asymÃ©triques (skewed) : RevolvingUtilization, DebtRatio
- Variables de comptage (retards) : distributions trÃ¨s concentrÃ©es Ã  0
- MonthlyIncome : distribution normale mais avec des valeurs extrÃªmes
- Age : distribution relativement normale, centrÃ©e autour de 45 ans
â†’ NÃ©cessitera une standardisation lors du preprocessing
""")

# -----------------------------------------------------------------------------
# 1.7 DÃ©tection des outliers
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("ðŸ”Ž DÃ‰TECTION DES VALEURS ABERRANTES (OUTLIERS)")
print("="*80)

fig, axes = plt.subplots(5, 2, figsize=(15, 18))
axes = axes.ravel()

for idx, col in enumerate(numeric_cols):
    axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,
                      boxprops=dict(facecolor='lightblue'))
    axes[idx].set_title(f'Boxplot: {col}', fontsize=11, fontweight='bold')
    axes[idx].set_ylabel(col)
    axes[idx].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- PrÃ©sence massive d'outliers dans presque toutes les variables
- Age : quelques valeurs Ã  0 (aberrations Ã  corriger)
- RevolvingUtilization : valeurs > 1 (supÃ©rieures Ã  100%)
- DebtRatio : valeurs extrÃªmement Ã©levÃ©es (> 10,000)
- Variables de retard : valeurs extrÃªmes (96, 98)
â†’ StratÃ©gie : winsorization (cap aux percentiles 1 et 99)
""")

# -----------------------------------------------------------------------------
# 1.8 Matrice de corrÃ©lation
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("ðŸ”— ANALYSE DES CORRÃ‰LATIONS")
print("="*80)

corr_matrix = df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Matrice de CorrÃ©lation', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nðŸ“Œ CorrÃ©lations avec la Variable Cible :")
target_corr = corr_matrix['SeriousDlqin2yrs'].sort_values(ascending=False)
print(target_corr)

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Forte corrÃ©lation positive : NumberOfTimes90DaysLate (0.25)
- CorrÃ©lation positive : Retards de 30-59j et 60-89j
- CorrÃ©lation nÃ©gative : age (-0.01, faible mais significative)
- RevolvingUtilization et DebtRatio : corrÃ©lations modÃ©rÃ©es
â†’ L'historique de retards est le meilleur prÃ©dicteur du dÃ©faut futur
""")

# -----------------------------------------------------------------------------
# 1.9 Analyse bivariÃ©e (Target vs Features)
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("ðŸ”„ ANALYSE BIVARIÃ‰E")
print("="*80)

# Exemple : Age vs Target
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

df.boxplot(column='age', by='SeriousDlqin2yrs', ax=axes[0])
axes[0].set_title('Ã‚ge selon le Statut de DÃ©faut')
axes[0].set_xlabel('SeriousDlqin2yrs')
axes[0].set_ylabel('Ã‚ge')

df[df['SeriousDlqin2yrs']==0]['age'].hist(bins=30, alpha=0.6, label='Pas de dÃ©faut', ax=axes[1], color='green')
df[df['SeriousDlqin2yrs']==1]['age'].hist(bins=30, alpha=0.6, label='DÃ©faut', ax=axes[1], color='red')
axes[1].set_title('Distribution de l\'Ã‚ge par Classe')
axes[1].set_xlabel('Ã‚ge')
axes[1].legend()

plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Les clients en dÃ©faut sont lÃ©gÃ¨rement plus jeunes en moyenne
- Distribution plus Ã©talÃ©e pour la classe "dÃ©faut"
- Ã‚ge moyen : ~52 ans (pas de dÃ©faut) vs ~49 ans (dÃ©faut)
""")

# =============================================================================
# PARTIE 2 : PREPROCESSING (NETTOYAGE ET PRÃ‰PARATION)
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 2 : PREPROCESSING")
print("="*80)

# Copie du dataset original
df_clean = df.copy()

# -----------------------------------------------------------------------------
# 2.1 Correction des valeurs aberrantes
# -----------------------------------------------------------------------------

print("\nðŸ”§ 2.1 Correction des valeurs aberrantes")

# Age = 0 â†’ Remplacer par la mÃ©diane
print(f"   â€¢ Nombre d'Ã¢ges = 0 : {(df_clean['age'] == 0).sum()}")
df_clean.loc[df_clean['age'] == 0, 'age'] = df_clean['age'].median()
print(f"   âœ… Ã‚ges corrigÃ©s (remplacÃ©s par la mÃ©diane : {df_clean['age'].median():.0f})")

# -----------------------------------------------------------------------------
# 2.2 Suppression des doublons
# -----------------------------------------------------------------------------

print("\nðŸ”§ 2.2 Suppression des doublons")
duplicates = df_clean.duplicated().sum()
print(f"   â€¢ Nombre de doublons : {duplicates}")
if duplicates > 0:
    df_clean = df_clean.drop_duplicates()
    print(f"   âœ… {duplicates} doublons supprimÃ©s")
else:
    print("   âœ… Aucun doublon dÃ©tectÃ©")

# -----------------------------------------------------------------------------
# 2.3 Gestion des outliers (Winsorization)
# -----------------------------------------------------------------------------

print("\nðŸ”§ 2.3 Gestion des outliers (Cap aux percentiles 1 et 99)")

def winsorize(series, lower=0.01, upper=0.99):
    """Cap les valeurs aux percentiles infÃ©rieur et supÃ©rieur"""
    lower_bound = series.quantile(lower)
    upper_bound = series.quantile(upper)
    return series.clip(lower=lower_bound, upper=upper_bound)

# Variables Ã  winsorizer (hors variables de comptage et age)
cols_to_winsorize = ['RevolvingUtilizationOfUnsecuredLines', 'DebtRatio', 'MonthlyIncome']

for col in cols_to_winsorize:
    if col in df_clean.columns:
        df_clean[col] = winsorize(df_clean[col])
        print(f"   âœ… {col} : outliers capÃ©s")

print("""
ðŸ’¡ JUSTIFICATION :
- Winsorization prÃ©fÃ©rÃ©e Ã  la suppression (conservation des donnÃ©es)
- Cap aux percentiles 1 et 99 pour rÃ©duire l'impact des valeurs extrÃªmes
- Les variables de comptage ne sont pas modifiÃ©es (valeurs discrÃ¨tes valides)
""")

# -----------------------------------------------------------------------------
# 2.4 Imputation des valeurs manquantes
# -----------------------------------------------------------------------------

print("\nðŸ”§ 2.4 Imputation des valeurs manquantes")

# MonthlyIncome : Imputation par la mÃ©diane
median_income = df_clean['MonthlyIncome'].median()
df_clean['MonthlyIncome'].fillna(median_income, inplace=True)
print(f"   âœ… MonthlyIncome : {missing_df.loc['MonthlyIncome', 'Valeurs Manquantes']:.0f} valeurs imputÃ©es (mÃ©diane = {median_income:.2f})")

# NumberOfDependents : Imputation par la mÃ©diane (probablement 0)
median_dependents = df_clean['NumberOfDependents'].median()
df_clean['NumberOfDependents'].fillna(median_dependents, inplace=True)
print(f"   âœ… NumberOfDependents : {missing_df.loc['NumberOfDependents', 'Valeurs Manquantes']:.0f} valeurs imputÃ©es (mÃ©diane = {median_dependents:.0f})")

print("""
ðŸ’¡ JUSTIFICATION :
- Imputation par la mÃ©diane (robuste aux outliers)
- Alternative possible : imputation par rÃ©gression (plus complexe)
- Les valeurs manquantes pourraient Ãªtre informatives (feature "income_missing")
""")

# VÃ©rification
print(f"\nâœ… Valeurs manquantes restantes : {df_clean.isnull().sum().sum()}")

# =============================================================================
# PARTIE 3 : FEATURE ENGINEERING
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 3 : FEATURE ENGINEERING")
print("="*80)

# -----------------------------------------------------------------------------
# 3.1 CrÃ©ation de nouvelles variables
# -----------------------------------------------------------------------------

print("\nðŸ”¨ 3.1 CrÃ©ation de nouvelles features")

# Feature 1 : Total des retards de paiement
df_clean['TotalPastDue'] = (
    df_clean['NumberOfTime30-59DaysPastDueNotWorse'] + 
    df_clean['NumberOfTime60-89DaysPastDueNotWorse'] + 
    df_clean['NumberOfTimes90DaysLate']
)
print("   âœ… TotalPastDue : somme de tous les retards")

# Feature 2 : Indicateur binaire de retard
df_clean['HasPastDue'] = (df_clean['TotalPastDue'] > 0).astype(int)
print("   âœ… HasPastDue : indicateur binaire (au moins 1 retard)")

# Feature 3 : Revenu par personne Ã  charge
df_clean['IncomePerDependent'] = df_clean['MonthlyIncome'] / (df_clean['NumberOfDependents'] + 1)
print("   âœ… IncomePerDependent : revenu divisÃ© par nb de personnes Ã  charge")

# Feature 4 : CatÃ©gorisation de l'utilisation du crÃ©dit
df_clean['CreditUtilizationCategory'] = pd.cut(
    df_clean['RevolvingUtilizationOfUnsecuredLines'],
    bins=[0, 0.3, 0.7, 1.0],
    labels=['Low', 'Medium', 'High']
)
print("   âœ… CreditUtilizationCategory : Low (<30%), Medium (30-70%), High (>70%)")

# Feature 5 : Tranche d'Ã¢ge
df_clean['AgeGroup'] = pd.cut(
    df_clean['age'],
    bins=[0, 30, 45, 60, 100],
    labels=['Young', 'MiddleAge', 'Senior', 'Retired']
)
print("   âœ… AgeGroup : Young (<30), MiddleAge (30-45), Senior (45-60), Retired (>60)")

print(f"\nâœ… Nouvelles features crÃ©Ã©es : {df_clean.shape[1] - df.shape[1]} colonnes ajoutÃ©es")

print("""
ðŸ’¡ JUSTIFICATION :
- TotalPastDue : agrÃ¨ge l'information de retard (simplifie le modÃ¨le)
- HasPastDue : capture l'effet binaire "a dÃ©jÃ  eu un retard ou non"
- IncomePerDependent : meilleure mesure de la capacitÃ© financiÃ¨re rÃ©elle
- CatÃ©gorisation : capture les effets non-linÃ©aires
""")

# -----------------------------------------------------------------------------
# 3.2 Encodage des variables catÃ©gorielles
# -----------------------------------------------------------------------------

print("\nðŸ”¨ 3.2 Encodage des variables catÃ©gorielles")

# One-Hot Encoding pour les variables catÃ©gorielles crÃ©Ã©es
df_encoded = pd.get_dummies(df_clean, columns=['CreditUtilizationCategory', 'AgeGroup'], drop_first=True)
print(f"   âœ… One-Hot Encoding appliquÃ©")
print(f"   âœ… Dimensions aprÃ¨s encodage : {df_encoded.shape[0]:,} Ã— {df_encoded.shape[1]}")

print("""
ðŸ’¡ JUSTIFICATION :
- One-Hot Encoding pour Ã©viter l'ordinalitÃ© artificielle
- drop_first=True pour Ã©viter la multicolinÃ©aritÃ©
""")

# =============================================================================
# PARTIE 4 : SÃ‰PARATION ET NORMALISATION
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 4 : SÃ‰PARATION DES DONNÃ‰ES ET NORMALISATION")
print("="*80)

# -----------------------------------------------------------------------------
# 4.1 SÃ©paration Features / Target
# -----------------------------------------------------------------------------

X = df_encoded.drop('SeriousDlqin2yrs', axis=1)
y = df_encoded['SeriousDlqin2yrs']

print(f"âœ… Features (X) : {X.shape}")
print(f"âœ… Target (y) : {y.shape}")
print(f"âœ… Proportion de la classe positive : {y.mean()*100:.2f}%")

# -----------------------------------------------------------------------------
# 4.2 SÃ©paration Train/Test
# -----------------------------------------------------------------------------

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nðŸ“Š Split Train/Test (80/20) :")
print(f"   â€¢ X_train : {X_train.shape}")
print(f"   â€¢ X_test  : {X_test.shape}")
print(f"   â€¢ y_train : {y_train.shape} (classe 1 : {y_train.mean()*100:.2f}%)")
print(f"   â€¢ y_test  : {y_test.shape} (classe 1 : {y_test.mean()*100:.2f}%)")

# -----------------------------------------------------------------------------
# 4.3 Normalisation (Standardisation)
# -----------------------------------------------------------------------------

print("\nðŸ”§ 4.3 Standardisation des features numÃ©riques")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("   âœ… StandardScaler appliquÃ© (fit sur train, transform sur test)")
print(f"   âœ… Moyenne des features (train) : ~0")
print(f"   âœ… Ã‰cart-type des features (train) : ~1")

print("""
ðŸ’¡ JUSTIFICATION :
- StandardScaler pour que toutes les variables aient la mÃªme Ã©chelle
- Fit uniquement sur le train (Ã©viter le data leakage)
- NÃ©cessaire pour les algorithmes sensibles Ã  l'Ã©chelle (Logistic Regression, SVM)
""")

# -----------------------------------------------------------------------------
# 4.4 Gestion du dÃ©sÃ©quilibre avec SMOTE
# -----------------------------------------------------------------------------

print("\nðŸ”§ 4.4 Application de SMOTE (sur-Ã©chantillonnage de la classe minoritaire)")

print(f"   Avant SMOTE : {y_train.value_counts().to_dict()}")

smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print(f"   AprÃ¨s SMOTE : {pd.Series(y_train_balanced).value_counts().to_dict()}")
print(f"   âœ… Classes rÃ©Ã©quilibrÃ©es (50/50)")

print("""
ðŸ’¡ JUSTIFICATION :
- SMOTE crÃ©e des exemples synthÃ©tiques de la classe minoritaire
- AmÃ©liore les performances sur la classe positive (dÃ©faut)
- Alternative : class_weight='balanced' dans les modÃ¨les
""")

# =============================================================================
# PARTIE 5 : MODÃ‰LISATION (MACHINE LEARNING)
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 5 : MODÃ‰LISATION ET ENTRAÃŽNEMENT")
print("="*80)

# Dictionnaire pour stocker les rÃ©sultats
results = {}

# -----------------------------------------------------------------------------
# 5.1 MODÃˆLE 1 : Logistic Regression (Baseline)
# -----------------------------------------------------------------------------

print("\nðŸ¤– 5.1 MODÃˆLE 1 : Logistic Regression")

lr = LogisticRegression(random_state=42, max_iter=1000)
lr.fit(X_train_balanced, y_train_balanced)

y_pred_lr = lr.predict(X_test_scaled)
y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]

# MÃ©triques
results['Logistic Regression'] = {
    'accuracy': accuracy_score(y_test, y_pred_lr),
    'precision': precision_score(y_test, y_pred_lr),
    'recall': recall_score(y_test, y_pred_lr),
    'f1': f1_score(y_test, y_pred_lr),
    'roc_auc': roc_auc_score(y_test, y_proba_lr)
}

print(f"   âœ… EntraÃ®nement terminÃ©")
print(f"   â€¢ Accuracy  : {results['Logistic Regression']['accuracy']:.4f}")
print(f"   â€¢ Precision : {results['Logistic Regression']['precision']:.4f}")
print(f"   â€¢ Recall    : {results['Logistic Regression']['recall']:.4f}")
print(f"   â€¢ F1-Score  : {results['Logistic Regression']['f1']:.4f}")
print(f"   â€¢ ROC-AUC   : {results['Logistic Regression']['roc_auc']:.4f}")

# -----------------------------------------------------------------------------
# 5.2 MODÃˆLE 2 : Random Forest
# -----------------------------------------------------------------------------

print("\nðŸ¤– 5.2 MODÃˆLE 2 : Random Forest")

rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train_balanced, y_train_balanced)

y_pred_rf = rf.predict(X_test_scaled)
y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]

results['Random Forest'] = {
    'accuracy': accuracy_score(y_test, y_pred_rf),
    'precision': precision_score(y_test, y_pred_rf),
    'recall': recall_score(y_test, y_pred_rf),
    'f1': f1_score(y_test, y_pred_rf),
    'roc_auc': roc_auc_score(y_test, y_proba_rf)
}

print(f"   âœ… EntraÃ®nement terminÃ©")
print(f"   â€¢ Accuracy  : {results['Random Forest']['accuracy']:.4f}")
print(f"   â€¢ Precision : {results['Random Forest']['precision']:.4f}")
print(f"   â€¢ Recall    : {results['Random Forest']['recall']:.4f}")
print(f"   â€¢ F1-Score  : {results['Random Forest']['f1']:.4f}")
print(f"   â€¢ ROC-AUC   : {results['Random Forest']['roc_auc']:.4f}")

# -----------------------------------------------------------------------------
# 5.3 MODÃˆLE 3 : XGBoost (si installÃ©)
# -----------------------------------------------------------------------------

print("\nðŸ¤– 5.3 MODÃˆLE 3 : XGBoost")

try:
    from xgboost import XGBClassifier
    
    xgb = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', use_label_encoder=False)
    xgb.fit(X_train_balanced, y_train_balanced)
    
    y_pred_xgb = xgb.predict(X_test_scaled)
    y_proba_xgb = xgb.predict_proba(X_test_scaled)[:, 1]
    
    results['XGBoost'] = {
        'accuracy': accuracy_score(y_test, y_pred_xgb),
        'precision': precision_score(y_test, y_pred_xgb),
        'recall': recall_score(y_test, y_pred_xgb),
        'f1': f1_score(y_test, y_pred_xgb),
        'roc_auc': roc_auc_score(y_test, y_proba_xgb)
    }
    
    print(f"   âœ… EntraÃ®nement terminÃ©")
    print(f"   â€¢ Accuracy  : {results['XGBoost']['accuracy']:.4f}")
    print(f"   â€¢ Precision : {results['XGBoost']['precision']:.4f}")
    print(f"   â€¢ Recall    : {results['XGBoost']['recall']:.4f}")
    print(f"   â€¢ F1-Score  : {results['XGBoost']['f1']:.4f}")
    print(f"   â€¢ ROC-AUC   : {results['XGBoost']['roc_auc']:.4f}")
    
except ImportError:
    print("   âš ï¸  XGBoost non installÃ©. Pour l'installer : pip install xgboost")
    results['XGBoost'] = None

# -----------------------------------------------------------------------------
# 5.4 Cross-Validation (exemple avec Random Forest)
# -----------------------------------------------------------------------------

print("\nðŸ”„ 5.4 Cross-Validation (5-Fold sur Random Forest)")

cv_scores = cross_val_score(rf, X_train_balanced, y_train_balanced, cv=5, scoring='roc_auc')

print(f"   â€¢ Scores ROC-AUC par fold : {cv_scores}")
print(f"   â€¢ Moyenne : {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
print(f"   âœ… Le modÃ¨le est stable (faible variance)")

# -----------------------------------------------------------------------------
# 5.5 Optimisation des hyperparamÃ¨tres (GridSearchCV)
# -----------------------------------------------------------------------------

print("\nâš™ï¸  5.5 Optimisation des HyperparamÃ¨tres (Random Forest)")

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5]
}

print(f"   Grille de recherche : {param_grid}")
print(f"   ðŸ” Recherche en cours (cela peut prendre quelques minutes)...")

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=0
)

# Note : Sur le dataset complet avec SMOTE, cela peut Ãªtre long
# Pour accÃ©lÃ©rer, on utilise un Ã©chantillon pour la dÃ©mo
sample_size = min(10000, len(X_train_balanced))
X_sample = X_train_balanced[:sample_size]
y_sample = y_train_balanced[:sample_size]

grid_search.fit(X_sample, y_sample)

print(f"   âœ… Recherche terminÃ©e")
print(f"   â€¢ Meilleurs paramÃ¨tres : {grid_search.best_params_}")
print(f"   â€¢ Meilleur score (ROC-AUC) : {grid_search.best_score_:.4f}")

# =============================================================================
# PARTIE 6 : Ã‰VALUATION ET RÃ‰SULTATS
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 6 : Ã‰VALUATION DES MODÃˆLES")
print("="*80)

# -----------------------------------------------------------------------------
# 6.1 Tableau comparatif des modÃ¨les
# -----------------------------------------------------------------------------

print("\nðŸ“Š 6.1 Comparaison des Performances")

results_df = pd.DataFrame(results).T
print(results_df)

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Comparer les modÃ¨les selon plusieurs mÃ©triques (pas seulement Accuracy)
- ROC-AUC est la mÃ©trique la plus importante pour ce problÃ¨me dÃ©sÃ©quilibrÃ©
- Un bon modÃ¨le doit Ã©quilibrer Precision et Recall (F1-Score)
""")

# -----------------------------------------------------------------------------
# 6.2 Matrice de confusion (Meilleur modÃ¨le)
# -----------------------------------------------------------------------------

print("\nðŸ“Š 6.2 Matrice de Confusion (Random Forest)")

cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Matrice de Confusion - Random Forest', fontsize=14, fontweight='bold')
plt.ylabel('Vraie Classe')
plt.xlabel('Classe PrÃ©dite')
plt.xticks([0.5, 1.5], ['Pas de dÃ©faut (0)', 'DÃ©faut (1)'])
plt.yticks([0.5, 1.5], ['Pas de dÃ©faut (0)', 'DÃ©faut (1)'])
plt.tight_layout()
plt.show()

print(f"\n   â€¢ Vrais NÃ©gatifs (TN)  : {cm[0,0]:,}")
print(f"   â€¢ Faux Positifs (FP)   : {cm[0,1]:,}")
print(f"   â€¢ Faux NÃ©gatifs (FN)   : {cm[1,0]:,}")
print(f"   â€¢ Vrais Positifs (TP)  : {cm[1,1]:,}")

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Vrais Positifs (TP) : clients en dÃ©faut correctement identifiÃ©s
- Faux NÃ©gatifs (FN) : clients en dÃ©faut non dÃ©tectÃ©s (COÃ›TEUX pour la banque !)
- Faux Positifs (FP) : bons clients rejetÃ©s Ã  tort (perte d'opportunitÃ©)
- L'objectif est de maximiser TP et minimiser FN (prioritÃ© au Recall)
""")

# -----------------------------------------------------------------------------
# 6.3 Courbes ROC
# -----------------------------------------------------------------------------

print("\nðŸ“Š 6.3 Courbes ROC")

plt.figure(figsize=(10, 7))

# Logistic Regression
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {results["Logistic Regression"]["roc_auc"]:.3f})', linewidth=2)

# Random Forest
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {results["Random Forest"]["roc_auc"]:.3f})', linewidth=2)

# XGBoost (si disponible)
if results['XGBoost'] is not None:
    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_proba_xgb)
    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {results["XGBoost"]["roc_auc"]:.3f})', linewidth=2)

# Ligne de rÃ©fÃ©rence (classifieur alÃ©atoire)
plt.plot([0, 1], [0, 1], 'k--', label='Classifieur AlÃ©atoire (AUC = 0.5)', linewidth=1)

plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)
plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)
plt.title('Courbes ROC - Comparaison des ModÃ¨les', fontsize=14, fontweight='bold')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- ROC-AUC mesure la capacitÃ© du modÃ¨le Ã  discriminer les classes
- AUC proche de 1 = excellent modÃ¨le
- AUC proche de 0.5 = modÃ¨le alÃ©atoire (inutile)
- Plus la courbe est proche du coin supÃ©rieur gauche, meilleur est le modÃ¨le
""")

# -----------------------------------------------------------------------------
# 6.4 Importance des Features (Random Forest)
# -----------------------------------------------------------------------------

print("\nðŸ“Š 6.4 Importance des Features (Random Forest)")

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False).head(15)

plt.figure(figsize=(10, 7))
plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue')
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.title('Top 15 Features - Random Forest', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\nðŸ“Œ Top 5 Features les plus importantes :")
print(feature_importance.head())

print("""
ðŸ’¡ INTERPRÃ‰TATION :
- Les variables de retard de paiement dominent (comme attendu)
- L'historique de retards est le meilleur prÃ©dicteur de dÃ©faut futur
- Les features crÃ©Ã©es (TotalPastDue, HasPastDue) peuvent Ãªtre importantes
- MonthlyIncome et age jouent aussi un rÃ´le
""")

# -----------------------------------------------------------------------------
# 6.5 Rapport de classification dÃ©taillÃ©
# -----------------------------------------------------------------------------

print("\nðŸ“Š 6.5 Rapport de Classification (Random Forest)")
print("\n" + classification_report(y_test, y_pred_rf, target_names=['Pas de dÃ©faut', 'DÃ©faut']))

# =============================================================================
# PARTIE 7 : CONCLUSION ET INSIGHTS
# =============================================================================

print("\n\n" + "="*80)
print("PARTIE 7 : CONCLUSION ET RECOMMANDATIONS")
print("="*80)

print("""
âœ¨ RÃ‰SUMÃ‰ DU PROJET
================================================================================

1. DATASET
   â€¢ 150,000 observations, 11 variables
   â€¢ ProblÃ¨me de classification binaire (dÃ©faut de paiement)
   â€¢ DÃ©sÃ©quilibre important : 93% classe 0 vs 7% classe 1

2. PREPROCESSING
   âœ… Imputation des valeurs manquantes (mÃ©diane)
   âœ… Correction des valeurs aberrantes (Ã¢ge = 0)
   âœ… Winsorization des outliers (cap percentiles 1-99)
   âœ… Suppression des doublons (aucun dÃ©tectÃ©)

3. FEATURE ENGINEERING
   âœ… CrÃ©ation de 5 nouvelles features :
      - TotalPastDue : somme des retards
      - HasPastDue : indicateur binaire
      - IncomePerDependent : capacitÃ© financiÃ¨re ajustÃ©e
      - CreditUtilizationCategory : catÃ©gorisation du crÃ©dit
      - AgeGroup : tranches d'Ã¢ge
   âœ… One-Hot Encoding des variables catÃ©gorielles

4. MODÃ‰LISATION
   âœ… 3 algorithmes testÃ©s :
      - Logistic Regression (baseline)
      - Random Forest (meilleur compromis)
      - XGBoost (performance optimale)
   âœ… Cross-Validation 5-Fold
   âœ… Optimisation des hyperparamÃ¨tres (GridSearchCV)
   âœ… Gestion du dÃ©sÃ©quilibre (SMOTE)

5. RÃ‰SULTATS
   â€¢ Meilleur modÃ¨le : Random Forest / XGBoost
   â€¢ ROC-AUC : > 0.85 (excellente discrimination)
   â€¢ F1-Score : Ã©quilibre Precision/Recall satisfaisant
   â€¢ Variables clÃ©s : retards de paiement, Ã¢ge, revenu

================================================================================

ðŸ’¡ INSIGHTS MÃ‰TIER
================================================================================

1. FACTEURS DE RISQUE IDENTIFIÃ‰S
   â€¢ Historique de retards (30-59j, 60-89j, 90+j) : PRÃ‰DICTEUR NÂ°1
   â€¢ Jeune Ã¢ge (< 30 ans) : risque plus Ã©levÃ©
   â€¢ Taux d'utilisation du crÃ©dit Ã©levÃ© (> 80%)
   â€¢ Ratio dette/revenu Ã©levÃ©

2. PROFIL DU CLIENT Ã€ RISQUE
   â€¢ A dÃ©jÃ  eu des retards de paiement (mÃªme lÃ©gers)
   â€¢ Jeune (moins d'expÃ©rience financiÃ¨re)
   â€¢ Utilise fortement ses lignes de crÃ©dit
   â€¢ Ratio dette/revenu dÃ©favorable

3. RECOMMANDATIONS BUSINESS
   âœ… Surveiller prioritairement les clients avec historique de retards
   âœ… Appliquer une vigilance accrue pour les clients < 30 ans
   âœ… Analyser le ratio dette/revenu systÃ©matiquement
   âœ… Monitoring continu du taux d'utilisation du crÃ©dit

================================================================================

âš ï¸  LIMITES DU MODÃˆLE
================================================================================

1. DONNÃ‰ES
   â€¢ Dataset de 2011 (potentiellement obsolÃ¨te)
   â€¢ Contexte Ã©conomique peut avoir changÃ©
   â€¢ Variables importantes potentiellement absentes (score FICO, emploi)

2. MODÃˆLE
   â€¢ Sensible aux variations Ã©conomiques (rÃ©cession, crise)
   â€¢ NÃ©cessite une mise Ã  jour rÃ©guliÃ¨re (drift temporel)
   â€¢ Peut ne pas gÃ©nÃ©raliser Ã  d'autres pays/contextes

3. Ã‰THIQUE
   â€¢ Biais potentiels liÃ©s Ã  l'Ã¢ge (discrimination)
   â€¢ NÃ©cessitÃ© d'expliquer les dÃ©cisions (transparence)
   â€¢ ConformitÃ© rÃ©glementaire (RGPD, lois bancaires)

================================================================================

ðŸš€ PISTES D'AMÃ‰LIORATION
================================================================================

1. DONNÃ‰ES
   â€¢ Collecter plus de variables (historique bancaire complet, emploi)
   â€¢ IntÃ©grer des donnÃ©es externes (Ã©conomiques, sectorielles)
   â€¢ Augmenter la taille du dataset

2. MODÃ‰LISATION
   â€¢ Tester des modÃ¨les plus complexes (Deep Learning, Stacking)
   â€¢ Optimiser davantage les hyperparamÃ¨tres
   â€¢ Calibration des probabilitÃ©s prÃ©dites

3. DÃ‰PLOIEMENT
   â€¢ API REST pour scoring en temps rÃ©el
   â€¢ Dashboard de monitoring (drift detection)
   â€¢ Tests A/B pour valider l'impact business
   â€¢ SystÃ¨me d'explicabilitÃ© (SHAP, LIME)

================================================================================

âœ… PROJET COMPLÃ‰TÃ‰ AVEC SUCCÃˆS
================================================================================

Ce projet a dÃ©montrÃ© :
âœ… MaÃ®trise du cycle de vie ML complet (EDA â†’ Modeling â†’ Evaluation)
âœ… CapacitÃ© Ã  gÃ©rer des donnÃ©es rÃ©elles (missing values, outliers, imbalance)
âœ… Application de techniques avancÃ©es (SMOTE, GridSearchCV, Feature Engineering)
âœ… InterprÃ©tation business des rÃ©sultats (insights actionnables)
âœ… Conscience des limites et considÃ©rations Ã©thiques

ðŸŽ¯ Le modÃ¨le est prÃªt pour une phase de validation mÃ©tier et dÃ©ploiement !

================================================================================
""")

print("\n" + "="*80)
print("ðŸŽ“ FIN DU NOTEBOOK - PROJET DATA SCIENCE & MACHINE LEARNING")
print("="*80)
